{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ikoghoemmanuell/Grocery-Store-Forecasting-Challenge-For-Azubian/blob/main/dev/notebooks/sales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0W08xH04Nzv"
      },
      "source": [
        "# Title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLTuR-lo4Nzx"
      },
      "source": [
        "# Description"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install category_encoders"
      ],
      "metadata": {
        "id": "Qmbb_sngwF8q",
        "outputId": "5c69a8b5-4f02-4c9c-bd96-6fdb35a2b1f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: category_encoders in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.22.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.10.1)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.13.5)\n",
            "Requirement already satisfied: pandas>=1.0.5 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (1.5.3)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /usr/local/lib/python3.10/dist-packages (from category_encoders) (0.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.5->category_encoders) (2022.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->category_encoders) (3.1.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.9.0->category_encoders) (23.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmdarima"
      ],
      "metadata": {
        "id": "tjMy6CatwySH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h76n_a9e4Nzy"
      },
      "source": [
        "# Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9KtdvmP4Nzz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import matplotlib.dates as mdates\n",
        "%matplotlib inline\n",
        "from itertools import product\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.stattools import kpss\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from category_encoders.binary import BinaryEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.api import AutoReg\n",
        "from pmdarima import auto_arima\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kfbniiz4Nz0"
      },
      "source": [
        "# Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3HFblb7_vMc"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the folder path in Google Drive where your CSV files are located\n",
        "folder_path = \"/content/drive/MyDrive/Colab Notebooks/datasets/grocery store azubian\"\n",
        "\n",
        "# Load the CSV files into DataFrames\n",
        "data = {}\n",
        "\n",
        "# Iterate over the files in the folder\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith(\".csv\"):\n",
        "        # Remove the file extension to get the variable name\n",
        "        variable_name = file_name.replace(\".csv\", \"\")\n",
        "\n",
        "        # Construct the file path\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "\n",
        "        # Read the CSV file content into a DataFrame\n",
        "        data[variable_name] = pd.read_csv(file_path)\n",
        "\n",
        "# Access the data using dictionary keysdri\n",
        "holidays = data[\"holidays\"]\n",
        "dates = data[\"dates\"]\n",
        "sample = data[\"SampleSubmission\"]\n",
        "stores = data[\"stores\"]\n",
        "test = data[\"test\"]\n",
        "train = data[\"train\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhVvJ2liwCwN"
      },
      "outputs": [],
      "source": [
        "# train = pd.read_csv(\"C:/Users/LENOVO/Music/Grocery-Store-Forecasting-Challenge-For-Azubian/assets/grocery store azubian/train.csv\")\n",
        "# test = pd.read_csv(\"C:/Users/LENOVO/Music/Grocery-Store-Forecasting-Challenge-For-Azubian/assets/grocery store azubian/test.csv\")\n",
        "# stores = pd.read_csv(\"C:/Users/LENOVO/Music/Grocery-Store-Forecasting-Challenge-For-Azubian/assets/grocery store azubian/stores.csv\")\n",
        "# sample = pd.read_csv(\"C:/Users/LENOVO/Music/Grocery-Store-Forecasting-Challenge-For-Azubian/assets/grocery store azubian/SampleSubmission.csv\")\n",
        "# dates = pd.read_csv(\"C:/Users/LENOVO/Music/Grocery-Store-Forecasting-Challenge-For-Azubian/assets/grocery store azubian/dates.csv\")\n",
        "# holidays = pd.read_csv(\"C:/Users/LENOVO/Music/Grocery-Store-Forecasting-Challenge-For-Azubian/assets/grocery store azubian/holidays.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mz9VjJVx4Nz3"
      },
      "source": [
        "# Dataset overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kajd5nDC490n"
      },
      "outputs": [],
      "source": [
        "train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKsCG3Cr5058"
      },
      "source": [
        "train date is in numerical format. We'll have to convert it to Datetime format later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBssAeqaAYbs"
      },
      "outputs": [],
      "source": [
        "test.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuTqIthgAuqV"
      },
      "outputs": [],
      "source": [
        "test.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0DDPOuEBLHK"
      },
      "outputs": [],
      "source": [
        "stores.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z_M2UuaBE_P"
      },
      "outputs": [],
      "source": [
        "stores.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJ_K9MGkLqCH"
      },
      "source": [
        "city, type & cluster are categoric variables, so they are not supposed to be in number datatype(int64)\n",
        "\n",
        "furthermore, cities do not have an ordinal relationship with one another. Ordinal variables have a natural order. Just like \"good-better-best\" or \"positive-neutral-negative\". Nominal variables don't.\n",
        "\n",
        "We don't want our machine learning models to think that one city-0 comes before city-1, which is before city-3.\n",
        "\n",
        "Therefore, we'll have to change the datatypes to obect or string to make it more descriptive, for example: 'London', 'Tokyo', 'Rome' and so on.\n",
        "\n",
        "Same goes for type and cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YAL7k5ynBny8"
      },
      "outputs": [],
      "source": [
        "dates['date'].unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G77f6HeABrDe"
      },
      "source": [
        "This dataset contains dates and the features that have already been extracted from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZR8aIxjxB26C"
      },
      "outputs": [],
      "source": [
        "dates.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWWEGqOHPmeh"
      },
      "source": [
        "in this case, these categories have an ordinal relationship with one another, meaning one date naturally comes before the other,\n",
        "\n",
        "so we can leave them as they are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db9Zn7fVQUI7"
      },
      "outputs": [],
      "source": [
        "dates.dayofyear.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOXxqaXsRn5M"
      },
      "source": [
        "we have 365 days in a year, 366 days is for a loop year. This is a problem for us. Let me explain why.\n",
        "\n",
        "**Problem**:\n",
        "When you have a loop year, then new year's eve would fall on day 366\n",
        "\n",
        "otherwise, it would fall on day 365. So everyday might not fall on the appropriate number for each year.\n",
        "\n",
        "**Solution**:\n",
        "we will later convert two new columns called \"sin(dayofyear)\" & \"cos(dayofyear)\". These new columns will help our machine learning models to understand the cyclic nature of a year.\n",
        "\n",
        "Cyclic means that a year usually starts and ends in a similar way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoPNhzliCQmv"
      },
      "outputs": [],
      "source": [
        "holidays.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWuH4aYBCX5s"
      },
      "outputs": [],
      "source": [
        "holidays.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cPnRlmkEdYHz"
      },
      "outputs": [],
      "source": [
        "holidays.type.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv1aEjzVc3Th"
      },
      "source": [
        "The type column is a categoric variable, and each type of holiday does not have an ordinal relationship, since a holiday like new year is not higher or better than Christmas for example.\n",
        "\n",
        "so we'll later convert them to string type to make it more descriptive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Slj_ROrsD0V4"
      },
      "outputs": [],
      "source": [
        "train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8ol2MBtEDkc"
      },
      "source": [
        "train dates range from **365** to **1626**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gefkrStbD9sK"
      },
      "outputs": [],
      "source": [
        "test.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NOEaiGeFM5f"
      },
      "source": [
        "test dates range from **1627** to **1682**\n",
        "\n",
        "this is a continuation from train. This makes sense since we are to predict future transactions based on past data\n",
        "\n",
        "**note**: we will not be using transaction data to train our models, since transaction data was not provided for our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1dIXNBJF2vX"
      },
      "outputs": [],
      "source": [
        "dates.describe(),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJq6eM6nGE8f"
      },
      "source": [
        "dates are from **365** till **1684** which covers the train and test dates\n",
        "\n",
        "so, we'll be able to add the features from here to both the train and test data based on the date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QPbCGdGGlIZ"
      },
      "outputs": [],
      "source": [
        " holidays.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjD4RclhaNLn"
      },
      "outputs": [],
      "source": [
        "# count the number of dates in the holidays dataset\n",
        "holidays.date.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vzexdncCW7J"
      },
      "source": [
        "notice that the dates in the holiday dataset are not complete\n",
        "\n",
        "so, we will later create a column for holidays in our train and test dataset based on the following logic:\n",
        "\n",
        "if a date is in the holidays table, then its a holiday, else that date is not a holiday"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PSAf8TQ4Nz6"
      },
      "source": [
        "## Hypothesis\n",
        "**H0**: holidays have a big effect on sales, hence the sales data is seasonal.\n",
        "\n",
        "**H1**: holidays don't affect sales, hence sales data is stationary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9m8lZeet4Nz6"
      },
      "source": [
        "## Questions\n",
        "\n",
        "1. Is the train data complete?\n",
        "2. Do we have seasonality in our sales?\n",
        "3. Are there outliers in our dataset?\n",
        "4. What is the difference between RMSLE, RMSE and MSE?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqYE8pYDyiT5"
      },
      "source": [
        "| Issues                                  | how we intend to solve them                                                                                                   |\n",
        "|----------------------------------------|------------------------------------------------------------------------------------------------------------|\n",
        "| 1. City, type & cluster in our stores dataset are mumerical | convert to string and make the categories more descriptive.                                            |\n",
        "| 2. The dayofyear column in our dates dataset ranges from 1 to 366. This will make some days fall on the wrong number | find the sine and cosine of this column to represent the cyclic nature of a year. | We can also include weather conditions, holidays and events to this.                        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZnWtLbK4Nz7"
      },
      "source": [
        "# Data Cleaning\n",
        "\n",
        "Here, we will prepare our data for Univariate and Bivariate analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VND4jVpS4Nz7"
      },
      "source": [
        "## Fixing our issues"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCQignpE4a_O"
      },
      "source": [
        "1. City, type & cluster in our stores dataset are mumerical\n",
        "\n",
        "Solution: convert to string and make the categories more descriptive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qEJuYQI-q2Q"
      },
      "source": [
        "**city**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORS8-rnS4hSA"
      },
      "outputs": [],
      "source": [
        "stores.city.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNBx10S-5XiM"
      },
      "outputs": [],
      "source": [
        "# using each city number as index,\n",
        "# convert each city number to the corresponding city from a list of us_cities\n",
        "stores.city = stores.city.apply(lambda x: 'city_'+ str(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K5CD29G-mip"
      },
      "outputs": [],
      "source": [
        "stores.city.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qIQ4Vcm-8yF"
      },
      "source": [
        "**type**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkHa2RMC--vR"
      },
      "outputs": [],
      "source": [
        "stores.type.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COckj6Bd_h0e"
      },
      "outputs": [],
      "source": [
        "# convert each store_type number to the corresponding store_type from a list of grocery_store_types\n",
        "stores.type = stores.type.apply(lambda x: 'store_'+ str(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDtCVODkAEkP"
      },
      "outputs": [],
      "source": [
        "stores.type.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5TqopxkAf1r"
      },
      "source": [
        "**cluster**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dH8INMmAiBC"
      },
      "outputs": [],
      "source": [
        "stores.cluster.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OM3bwR6BOyF"
      },
      "outputs": [],
      "source": [
        "# convert each cluster number to the corresponding cluster from a list of us_cities\n",
        "stores.cluster = stores.cluster.apply(lambda x: 'cluster_'+ str(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwik8MGXBuFp"
      },
      "outputs": [],
      "source": [
        "stores.cluster.unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5we9TFqwQel"
      },
      "outputs": [],
      "source": [
        "holidays.type.unique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSbfsjQKByfc"
      },
      "source": [
        "2. The dayofyear column in our dates dataset ranges from 1 to 366. This will make some days fall on the wrong number\n",
        "\n",
        "Solution: find the sine and cosine of this column to represent the cyclic nature of a year. We can also include weather conditions, holidays and events to this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05Z73hvCC2FN"
      },
      "outputs": [],
      "source": [
        "dates.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJFpNxN1B5xJ"
      },
      "outputs": [],
      "source": [
        "# create new coolumns to represent the cyclic nature of a year\n",
        "dates[\"sin(dayofyear)\"] = np.sin(dates[\"dayofyear\"])\n",
        "dates[\"cos(dayofyear)\"] = np.cos(dates[\"dayofyear\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sm0iN8DiwCxC"
      },
      "outputs": [],
      "source": [
        "def get_datetime(df):\n",
        "  # Create a new column combining the year, month, and day of the month in the desired format\n",
        "  df['date_extracted'] = (\n",
        "      dates['year'].astype(int).add(2000).astype(str) + '-' +\n",
        "      dates['month'].astype(str).str.zfill(2) + '-' +\n",
        "      dates['dayofmonth'].astype(str).str.zfill(2)\n",
        "  )\n",
        "\n",
        "get_datetime(dates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjFCEucJjUnj"
      },
      "source": [
        "### merging our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM3GgvlRwCxE"
      },
      "outputs": [],
      "source": [
        "stores.rename(  # rename type to store_type to make it more descriptive\n",
        "      columns={'type': 'store_type'},\n",
        "      inplace=True)\n",
        "holidays.rename(  # rename type to holiday_type to make it more descriptive\n",
        "      columns={'type': 'holiday_type'},\n",
        "      inplace=True)\n",
        "# make each holiday type a string\n",
        "holidays['holiday_type'] = holidays['holiday_type'].apply(lambda x: 'holiday_' + str(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKmk7PvkwCxE"
      },
      "outputs": [],
      "source": [
        "#merging train and test with stores dataset\n",
        "\n",
        "def merge(df1, df2):\n",
        "    merged_df = df1.merge(df2, how='left', on='date')\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "def merge_stores(df1, df2):\n",
        "    merged_df = df1.merge(df2, how='left', on='store_id')\n",
        "\n",
        "    return merged_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzLXDzmswCxH"
      },
      "outputs": [],
      "source": [
        "def get_is_holiday_column(df):\n",
        "  df['holiday_type'] = df['holiday_type'].fillna('Workday')\n",
        "\n",
        "  # create column to show if its a holiday or not (non-holidays are zeros)\n",
        "  df['is_holiday'] = df['holiday_type'].apply(\n",
        "      lambda x: False if x=='Workday'\n",
        "      else True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ua6F_k68ZzQ"
      },
      "source": [
        "we did this so our non-holidays can be zeros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEWAGxsV10J1"
      },
      "source": [
        "now we must merge holidays with the merged data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNnEVhP063uD"
      },
      "source": [
        "since non-holidays are zeros, we don't want our ML Models to think that non-holidays(zeros) have an ordinal relationship with other holidays(1,2,3,4,)\n",
        "\n",
        "in other words, non-holidays(zeros) don't always come before holidays(1,2,3,4,)\n",
        "\n",
        "so, we must create a new column to show whether or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WH3san4RwCxK"
      },
      "outputs": [],
      "source": [
        "train_merged = merge_stores(train, stores)\n",
        "train_merged1 = merge(train_merged, holidays)\n",
        "get_is_holiday_column(train_merged1)\n",
        "train_merged2 = merge(train_merged1, dates)\n",
        "\n",
        "test_merged = merge_stores(test, stores)\n",
        "test_merged1 = merge(test_merged, holidays)\n",
        "get_is_holiday_column(test_merged1)\n",
        "test_merged2 = merge(test_merged1, dates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ5tljFiwCxK"
      },
      "outputs": [],
      "source": [
        "train_merged2['holiday_type'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEguTT36wCxM"
      },
      "outputs": [],
      "source": [
        "# Convert the column to datetime with errors='coerce'\n",
        "train_merged2['date_ext'] = pd.to_datetime(train_merged2['date_extracted'], errors='coerce')\n",
        "\n",
        "# Filter rows with NaT values and convert them to a list\n",
        "invalid_dates = train_merged2.loc[train_merged2['date_ext'].isna(), 'date_extracted'].tolist()\n",
        "\n",
        "print(invalid_dates) #get a list of invalid dates\n",
        "print(list(set(invalid_dates))) #unique invalid dates\n",
        "train_merged2.drop('date_ext', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5j9W2IvwCxM"
      },
      "source": [
        "since the only invalid date is 2003-02-29, then when converting to datetime,\n",
        "\n",
        "we will first set invalid dates to NaT\n",
        "\n",
        "then fill them with 2003-02-29"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh9FOdxZwCxN"
      },
      "outputs": [],
      "source": [
        "train_merged2['date_extracted'] = pd.to_datetime(train_merged2['date_extracted'], errors='coerce')\n",
        "test_merged2['date_extracted'] = pd.to_datetime(test_merged2['date_extracted'])\n",
        "train_merged2['date_extracted'].fillna('2003-02-29')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2V51TaZwCxO"
      },
      "outputs": [],
      "source": [
        "def set_index(df):\n",
        "  df.drop('date', inplace=True, axis=1)\n",
        "  df.set_index('date_extracted', inplace=True)\n",
        "set_index(train_merged2)\n",
        "set_index(test_merged2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmZ1QBT54N0P"
      },
      "source": [
        "## Drop Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_muKXb1DvRi"
      },
      "outputs": [],
      "source": [
        "train_merged2.drop_duplicates(inplace=True)\n",
        "test_merged2.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsHWAQ6lwCxR"
      },
      "outputs": [],
      "source": [
        "train = train_merged2\n",
        "test = test_merged2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGnoifAP4N0P"
      },
      "source": [
        "## Impute Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d57DfF5_Jhaw"
      },
      "outputs": [],
      "source": [
        "print(train.isnull().sum())\n",
        "print(test.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIWpPR144N0Q"
      },
      "source": [
        "# Exploratory Data Analysis: EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD0uSjg0Mcck"
      },
      "source": [
        "## Hypothesis Validation\n",
        "**H0**: holidays have a big effect on sales, hence the sales data is seasonal.\n",
        "\n",
        "**H1**: holidays don't affect sales, hence sales data is stationary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeviX6Jfsd1o"
      },
      "outputs": [],
      "source": [
        "# Bar chart of sales by holiday type\n",
        "train.groupby('holiday_type')['target'].sum().plot(kind='bar')\n",
        "plt.xlabel('Holiday Type')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Total Sales by Holiday Type')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntlxx3X58KbB"
      },
      "outputs": [],
      "source": [
        "# Box plot of sales during holidays vs non-holidays\n",
        "train.boxplot(column='target', by='is_holiday', figsize=(8, 6))\n",
        "plt.xlabel('is_it_a_Holiday')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales During Holidays vs Non-Holidays')\n",
        "plt.suptitle('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6nLfhO94N0Q"
      },
      "source": [
        "## Answering Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6tpLgwhK20R"
      },
      "source": [
        "1. Is the train data complete?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Kf4QrNjLFSj"
      },
      "source": [
        "Yes. The output below shows that our train data is incomplete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANRYdnp8wCxV"
      },
      "outputs": [],
      "source": [
        "# create a function to check for missing extracted dates\n",
        "def get_missing_dates(df):\n",
        "  col = df.index\n",
        "  missing_dates = (pd.date_range(\n",
        "\n",
        "      start=col.min(), #start date\n",
        "      end=col.max())   #end_date\n",
        "      .difference(col))\n",
        "  print(f\"we have {len(missing_dates)} dates missing out of {len(col)}\")\n",
        "  return missing_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTdaJT8wK4gb"
      },
      "outputs": [],
      "source": [
        "get_missing_dates(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoPh2HC1wCxY"
      },
      "outputs": [],
      "source": [
        "get_missing_dates(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua7G3f6VK1hu"
      },
      "source": [
        "2. Do we have seasonality in our sales?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNTmmpjQC9d2"
      },
      "outputs": [],
      "source": [
        "# Assuming your time series data is stored in the variable 'sales_data'\n",
        "sales_data = train['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7udcLL2CDLjy"
      },
      "outputs": [],
      "source": [
        "# Perform KPSS test\n",
        "kpss_result = kpss(sales_data)\n",
        "kpss_statistic = kpss_result[0]\n",
        "kpss_pvalue = kpss_result[1]\n",
        "kpss_critical_values = kpss_result[3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OD16D9Ka_1DZ"
      },
      "outputs": [],
      "source": [
        "print(\"\\nKPSS Test:\")\n",
        "print(\"KPSS Statistic:\", kpss_statistic)\n",
        "print(\"p-value:\", kpss_pvalue)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tP01NM6wCxb"
      },
      "source": [
        "stationary if p-value > 0.05\n",
        "\n",
        "series is stationary since 0.01 < 0.05"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C-k2e6-gDEZP"
      },
      "outputs": [],
      "source": [
        "def check_stationarity(df, date_col, target_col, window=12):\n",
        "    # Calculate rolling statistics\n",
        "    rolling_std = df[target_col].rolling(window=window).std()\n",
        "    rolling_mean = df[target_col].rolling(window=window).mean()\n",
        "\n",
        "    # Plot original series and rolling statistics\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(df.index, df[target_col], color='blue', label='Original Series')\n",
        "    plt.plot(df.index, rolling_std, color='green', label='Rolling Std')\n",
        "    plt.plot(df.index, rolling_mean, color='red', label='Rolling Mean')\n",
        "    plt.legend()\n",
        "    plt.title('Rolling Statistics')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Target(sales)')\n",
        "    plt.tight_layout()  # Adjusts plot spacing\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "df = sales_data  # Assuming the sales data is stored in a dataframe called sales_data\n",
        "target_col = 'sales'  # Column containing the sales data\n",
        "\n",
        "check_stationarity(train, 'date_extracted', 'target')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va6nmkLbwCxc"
      },
      "source": [
        "### Checking for Stationarity of the Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxunYNWbwCxc"
      },
      "outputs": [],
      "source": [
        "# Perform seasonal decomposition\n",
        "result = seasonal_decompose(train['target'], model='additive', period=12)  # Adjust the period as needed\n",
        "\n",
        "# Plot the decomposed components\n",
        "fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(10, 8))\n",
        "result.observed.plot(ax=ax1)\n",
        "ax1.set_ylabel('Observed')\n",
        "result.trend.plot(ax=ax2)\n",
        "ax2.set_ylabel('Trend')\n",
        "result.seasonal.plot(ax=ax3)\n",
        "ax3.set_ylabel('Seasonal')\n",
        "result.resid.plot(ax=ax4)\n",
        "ax4.set_ylabel('Residual')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CmklbPFwCxc"
      },
      "source": [
        "Observed values: These are the actual values of the time series. They represent the data points that are observed or recorded over a period of time. In the context of sales data, the observed values would be the actual sales figures recorded at different time intervals.\n",
        "\n",
        "Trend: The trend component represents the long-term pattern or direction of the time series. It captures the underlying growth or decline in the data over an extended period. The trend component helps identify whether the series is increasing, decreasing, or remaining relatively stable over time.\n",
        "\n",
        "Seasonal: The seasonal component represents the periodic patterns or fluctuations that occur within a time series. It captures the regular and repetitive variations that happen within specific time periods, such as daily, weekly, monthly, or yearly cycles. In sales data, seasonal patterns may include higher sales during holiday seasons or lower sales during certain months of the year.\n",
        "\n",
        "Residual: The residual component, also known as the irregular or random component, represents the remaining variation in the time series after removing the trend and seasonal components. It includes any unpredictable or random fluctuations that are not accounted for by the trend or seasonal patterns. The residual component is often assumed to be noise or measurement error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h53wr-0FwCxd"
      },
      "outputs": [],
      "source": [
        "def time_plot(data, y_col, title):\n",
        "    fig, ax = plt.subplots(figsize=(15,5))\n",
        "    data.resample('M')[y_col].sum().plot(ax=ax, color='mediumblue', label='Total Sales')\n",
        "    data.resample('M')[y_col].mean().plot(ax=ax, color='red', label='Mean Sales')\n",
        "\n",
        "    ax.set(xlabel=\"Date\",\n",
        "           ylabel=\"Sales\",\n",
        "           title=title)\n",
        "\n",
        "    ax.legend()\n",
        "    sns.despine()\n",
        "\n",
        "# Example usage with your specific details\n",
        "time_plot(train, 'target', 'Monthly Sales Over the Years')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRYQFGKGK0Ny"
      },
      "source": [
        "3. Are there outliers in our dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO8E9bunKxvg"
      },
      "source": [
        "4. What is the difference between RMSLE, RMSE and MSE?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0ASY2Id4N0T"
      },
      "source": [
        "## Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lycj1Mjo4N0U"
      },
      "source": [
        "## Bivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPK3gua_wCxe"
      },
      "outputs": [],
      "source": [
        "# Calculate the correlation matrix\n",
        "correlation_matrix = train.corr()\n",
        "\n",
        "# Find the moderately correlated variables\n",
        "moderate_correlation = (correlation_matrix.abs() > 0.5) & (correlation_matrix != 1) & (correlation_matrix <0.8)\n",
        "\n",
        "# Get the variable pairs with moderate correlation\n",
        "moderate_correlation_pairs = [(i, j) for i in moderate_correlation.columns for j in moderate_correlation.columns if moderate_correlation.loc[i, j]]\n",
        "\n",
        "# Print the moderately correlated variables\n",
        "for pair in moderate_correlation_pairs:\n",
        "    var1, var2 = pair\n",
        "    correlation_value = correlation_matrix.loc[var1, var2]\n",
        "    print(f\"{var1} and {var2} are moderately correlated (correlation value: {correlation_value})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJyHn-cowCxe"
      },
      "source": [
        "These columns are all boolean, so let's look at others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRiodUOBwCxf"
      },
      "outputs": [],
      "source": [
        "# Set the threshold for high correlation\n",
        "threshold = 0.8\n",
        "\n",
        "# Find the highly correlated variables\n",
        "high_correlation = (correlation_matrix.abs() > threshold) & (correlation_matrix != 1)\n",
        "\n",
        "# Get the variable pairs with high correlation\n",
        "high_correlation_pairs = [(i, j) for i in high_correlation.columns for j in high_correlation.columns if high_correlation.loc[i, j]]\n",
        "\n",
        "# Print the highly correlated variables\n",
        "for pair in high_correlation_pairs:\n",
        "    var1, var2 = pair\n",
        "    correlation_value = correlation_matrix.loc[var1, var2]\n",
        "    print(f\"{var1} and {var2} are highly correlated (correlation value: {correlation_value})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6mAGzOQwCxf"
      },
      "outputs": [],
      "source": [
        "# Specify the column pairs and their correlation values\n",
        "column_pairs = [('year', 'year_weekofyear', 0.9884229388238451),\n",
        "                ('month', 'dayofyear', 0.9964919406599103),\n",
        "                ('month', 'weekofyear', 0.9658303008707717),\n",
        "                ('month', 'quarter', 0.9713815220940318),\n",
        "                ('dayofyear', 'month', 0.9964919406599103),\n",
        "                ('dayofyear', 'weekofyear', 0.9669203951023091),\n",
        "                ('dayofyear', 'quarter', 0.9685365398989686),\n",
        "                ('weekofyear', 'month', 0.9658303008707717),\n",
        "                ('weekofyear', 'dayofyear', 0.9669203951023091),\n",
        "                ('weekofyear', 'quarter', 0.9426460215490194),\n",
        "                ('quarter', 'month', 0.9713815220940318),\n",
        "                ('quarter', 'dayofyear', 0.9685365398989686),\n",
        "                ('quarter', 'weekofyear', 0.9426460215490194),\n",
        "                ('year_weekofyear', 'year', 0.9884229388238451)]\n",
        "\n",
        "# Create a grid layout for the scatter plots\n",
        "num_pairs = len(column_pairs)\n",
        "num_cols = 3  # Number of columns in the grid layout\n",
        "num_rows = (num_pairs + num_cols - 1) // num_cols  # Number of rows in the grid layout\n",
        "\n",
        "fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n",
        "\n",
        "# Create scatter plots for each column pair\n",
        "for i, pair in enumerate(column_pairs):\n",
        "    x_col, y_col, correlation_value = pair\n",
        "    row = i // num_cols\n",
        "    col = i % num_cols\n",
        "\n",
        "    # Select the appropriate subplot for the scatter plot\n",
        "    ax = axes[row, col] if num_rows > 1 else axes[col]\n",
        "\n",
        "    # Create the scatter plot\n",
        "    ax.scatter(train[x_col], train[y_col], alpha=0.5)\n",
        "    ax.set_xlabel(x_col)\n",
        "    ax.set_ylabel(y_col)\n",
        "    ax.set_title(f\"Scatter plot: {x_col} vs {y_col}\\nCorrelation value: {correlation_value:.4f}\")\n",
        "\n",
        "# Adjust the spacing between subplots\n",
        "fig.tight_layout()\n",
        "\n",
        "# Display the grid of scatter plots\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4REztMBZwCxg"
      },
      "source": [
        "Let's leave these because more date_features will help our ML models accuracy in this case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj467jzvwCxg"
      },
      "outputs": [],
      "source": [
        "# First format how figures apper in the notebook\n",
        "pd.options.display.float_format = '{:.2f}'.format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zw7mk8XZwCxg"
      },
      "source": [
        "**Summary of Our Sales and Number of Transactions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7impM6twCxg"
      },
      "outputs": [],
      "source": [
        "# Calculate summary statistics\n",
        "summary_stats = train[['target', 'nbr_of_transactions']].describe()\n",
        "print(summary_stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mK0Iv_5ewCxh"
      },
      "source": [
        "**Histogram of Sales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJehPW97wCxh"
      },
      "outputs": [],
      "source": [
        "# Histogram of 'target'\n",
        "train['target'].plot(kind='hist')\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Target')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfeEDlkgwCxh"
      },
      "source": [
        "**Correlation Between Sales and number of Transactions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3CGx0_MwCxi"
      },
      "outputs": [],
      "source": [
        "# Correlation matrix\n",
        "corr_matrix = train[['target', 'nbr_of_transactions']].corr()\n",
        "print(corr_matrix)\n",
        "\n",
        "# Scatter plot\n",
        "plt.scatter(train['target'], train['nbr_of_transactions'])\n",
        "plt.xlabel('Target')\n",
        "plt.ylabel('Number of Transactions')\n",
        "plt.title('Scatter Plot of Target vs Number of Transactions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_mRNrS7wCxi"
      },
      "source": [
        "The correlation coefficient between 'target' and 'nbr_of_transactions' is 0.24. This indicates a positive correlation between the two variables, but the correlation is relatively weak.\n",
        "\n",
        "It suggests that there is a weak tendency for the 'target' and 'nbr_of_transactions' to increase together, but the relationship is not very strong.\n",
        "\n",
        "Therefore, based on the correlation coefficient of 0.24, there is a weak positive correlation between the 'target' and 'nbr_of_transactions' columns in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDrRFyICwCxi"
      },
      "source": [
        "**Observing Sales over time**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k4tXhgijwCxi"
      },
      "outputs": [],
      "source": [
        "# Line plot of sales over time\n",
        "train['target'].plot()\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales Over Time')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oosP2Ux-wCxj"
      },
      "outputs": [],
      "source": [
        "# Resample the data by day and calculate the total sales for each day\n",
        "sales_daily = train['target'].resample('D').sum()\n",
        "\n",
        "# Create a line plot of the \"sales\" column\n",
        "plt.plot(sales_daily.index, sales_daily)\n",
        "\n",
        "# Set the title and axis labels\n",
        "plt.title(\"Total Sales by Day\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Total Sales\")\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7E5g5gBkwCxj"
      },
      "source": [
        "**Holiday Impact on Sales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8IfA3j-wCxj"
      },
      "outputs": [],
      "source": [
        "# Bar chart of sales by holiday type\n",
        "train.groupby('holiday_type')['target'].sum().plot(kind='bar')\n",
        "plt.xlabel('Holiday Type')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Total Sales by Holiday Type')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3VV6Ls5owCxk"
      },
      "outputs": [],
      "source": [
        "# Box plot of sales during holidays vs non-holidays\n",
        "train.boxplot(column='target', by='is_holiday', figsize=(8, 6))\n",
        "plt.xlabel('is_it_a_Holiday')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Sales During Holidays vs Non-Holidays')\n",
        "plt.suptitle('')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6EspfyVwCxk"
      },
      "source": [
        "**Stores Performance**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIZaL7XjwCxk"
      },
      "outputs": [],
      "source": [
        "# Bar chart of sales by store\n",
        "train.groupby('store_id')['target'].sum().plot(kind='bar')\n",
        "plt.xlabel('Store ID')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Total Sales by Store')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCT_aXEmwCxl"
      },
      "outputs": [],
      "source": [
        "# Bar chart of sales by category\n",
        "train.groupby('category_id')['target'].sum().plot(kind='bar')\n",
        "plt.xlabel('Category ID')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Total Sales by Category')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRCbIXRWwCxl"
      },
      "source": [
        "**Promotion Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fumn6KKwwCxl"
      },
      "outputs": [],
      "source": [
        "# Separate data for promotion and non-promotion\n",
        "promotion_data = train[train['onpromotion'] == 1]\n",
        "non_promotion_data = train[train['onpromotion'] == 0]\n",
        "\n",
        "# Calculate average sales per day for promotion and non-promotion\n",
        "promotion_avg_sales = promotion_data.groupby(promotion_data.index)['target'].mean()\n",
        "non_promotion_avg_sales = non_promotion_data.groupby(non_promotion_data.index)['target'].mean()\n",
        "\n",
        "# Line plot of average sales with and without promotion\n",
        "plt.plot(promotion_avg_sales.index, promotion_avg_sales, label='Promotion', color='blue')\n",
        "plt.plot(non_promotion_avg_sales.index, non_promotion_avg_sales, label='No Promotion', color='red')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Average Sales')\n",
        "plt.title('Average Sales with and without Promotion Over Time')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCloBhx4wCxm"
      },
      "source": [
        "**Monthly Statistics**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRbIqlhlwCxm"
      },
      "outputs": [],
      "source": [
        "# Filter data for the period from 2001 to 2003\n",
        "# sales_2001_to_2003 = train['2001':'2003']\n",
        "\n",
        "# Group by month and calculate sum of sales\n",
        "monthly_sales = train.groupby(train.index.month)['target'].sum()\n",
        "\n",
        "# Find the month with the highest sales\n",
        "highest_sales_month = monthly_sales.idxmax()\n",
        "\n",
        "# Print the month with the highest sales\n",
        "print(\"The month with the highest sales is:\", highest_sales_month)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O06vOQhfwCxo"
      },
      "outputs": [],
      "source": [
        "# Filter data for the period from 2001 to 2003\n",
        "# sales_2001_to_2003 = train['2001':'2003']\n",
        "\n",
        "# Group by month and calculate sum of sales\n",
        "monthly_sales = train.groupby(train.index.month)['target'].sum()\n",
        "\n",
        "# Find the month with the lowest sales\n",
        "lowest_sales_month = monthly_sales.idxmin()\n",
        "\n",
        "# Print the month with the lowest sales\n",
        "print(\"The month with the lowest sales is:\", lowest_sales_month)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJBqCoOcwCxp"
      },
      "outputs": [],
      "source": [
        "# Resample to monthly frequency and calculate sum of sales\n",
        "monthly_sales = train['target'].resample('M').sum()\n",
        "\n",
        "# Plot the monthly sales data\n",
        "monthly_sales.plot()\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Sales')\n",
        "plt.title('Monthly Sales Over the Years')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQyOpX6dM6Ig"
      },
      "source": [
        "## Multivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GmpRhjT4N0V"
      },
      "source": [
        "# Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDXigCJl4N0V"
      },
      "source": [
        "## Creating New Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnnEQBN9HfL3"
      },
      "outputs": [],
      "source": [
        "def getDateFeatures(df):\n",
        "\n",
        "    df[\"is_weekend\"] = df[\"dayofweek\"] > 4\n",
        "\n",
        "    # Define the criteria for each season\n",
        "    seasons = {'Winter': [12, 1, 2], 'Spring': [3, 4, 5], 'Summer': [6, 7, 8], 'Autumn': [9, 10, 11]}\n",
        "\n",
        "    # Create the 'season' column based on the 'date' column\n",
        "    df['season'] = df[\"month\"].map({month: season for season, months in seasons.items() for month in months})\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBC3kYEiHkEA"
      },
      "outputs": [],
      "source": [
        "getDateFeatures(train)\n",
        "getDateFeatures(test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_sum = train.groupby([pd.Grouper(freq='D'), 'store_id', 'category_id']).agg({'target': 'sum', 'onpromotion': 'sum', 'nbr_of_transactions': 'sum', 'city': 'first', 'store_type': 'first', 'cluster': 'first', 'holiday_type': 'first', 'is_holiday': 'first', 'year': 'first', 'month': 'first', 'dayofmonth': 'first', 'dayofweek': 'first', 'dayofyear': 'first', 'weekofyear': 'first', 'quarter': 'first', 'is_month_start': 'first', 'is_month_end': 'first', 'is_quarter_start': 'first', 'is_quarter_end': 'first', 'is_year_start': 'first', 'is_year_end': 'first', 'year_weekofyear': 'first', 'sin(dayofyear)': 'first', 'cos(dayofyear)': 'first', 'is_weekend': 'first', 'season': 'first'}).reset_index().set_index('date_extracted')\n",
        "train = weekly_sum\n",
        "weekly_sum1 = test.groupby([pd.Grouper(freq='D'), 'store_id', 'category_id']).agg({'onpromotion': 'sum', 'city': 'first', 'store_type': 'first', 'cluster': 'first', 'holiday_type': 'first', 'is_holiday': 'first', 'year': 'first', 'month': 'first', 'dayofmonth': 'first', 'dayofweek': 'first', 'dayofyear': 'first', 'weekofyear': 'first', 'quarter': 'first', 'is_month_start': 'first', 'is_month_end': 'first', 'is_quarter_start': 'first', 'is_quarter_end': 'first', 'is_year_start': 'first', 'is_year_end': 'first', 'year_weekofyear': 'first', 'sin(dayofyear)': 'first', 'cos(dayofyear)': 'first', 'is_weekend': 'first', 'season': 'first'}).reset_index().set_index('date_extracted')\n",
        "test = weekly_sum1"
      ],
      "metadata": {
        "id": "B4nQD-V7nBHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting relevant columns and creating ID column\n",
        "weekly_sum1['ID'] = 'year_week_' + weekly_sum1['year_weekofyear'].astype(str) + '_' + weekly_sum1['store_id'] + '_' + weekly_sum1['category_id']"
      ],
      "metadata": {
        "id": "H2YdJqP-jyht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FVGY23N4N0c"
      },
      "source": [
        "## Features Encoding & scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ub1-OAYM4N0d"
      },
      "outputs": [],
      "source": [
        "numeric_columns = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "categoric_columns = [col for col in train.columns if col not in numeric_columns]\n",
        "categoric_columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Tv6yroc4N0d"
      },
      "outputs": [],
      "source": [
        "numeric_columns.remove('target')\n",
        "numeric_columns.remove('nbr_of_transactions')\n",
        "# categoric_columns.remove('ID')\n",
        "print(numeric_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYA-XIKY4N0d"
      },
      "outputs": [],
      "source": [
        "encoder = BinaryEncoder(drop_invariant=False, return_df=True,)\n",
        "encoder.fit(train[categoric_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SisH2rrw4N0e"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.set_output(transform=\"pandas\")\n",
        "scaler.fit(train[numeric_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM4aITtn4N0e"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# with open('encoder.pkl', 'wb') as f:\n",
        "#     pickle.dump(encoder, f)\n",
        "\n",
        "# with open('scaler.pkl', 'wb') as f:\n",
        "#     pickle.dump(scaler, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVyirQUMwCxz"
      },
      "outputs": [],
      "source": [
        "scaled_num = scaler.transform(train[numeric_columns])\n",
        "scaled_num_test = scaler.transform(test[numeric_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "esOeOTQ1wCx0"
      },
      "outputs": [],
      "source": [
        "encoded_cat = encoder.transform(train[categoric_columns])\n",
        "encoded_cat_test = encoder.transform(test[categoric_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6C2kf5Zs4N0f"
      },
      "outputs": [],
      "source": [
        "train = pd.concat([scaled_num, encoded_cat, train['target']], axis=1)\n",
        "test = pd.concat([scaled_num_test, encoded_cat_test], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxw32giCwCxt"
      },
      "source": [
        "## Resampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G80z4W0iwCxu"
      },
      "outputs": [],
      "source": [
        "# resampled = train.resample('W').mean()\n",
        "# resampled_test = test.resample('W').mean()\n",
        "# train = resampled\n",
        "# test = resampled_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSoVN4uPwCxr"
      },
      "source": [
        "**dataframe for the traditional time series models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47I8fEFFwCxr"
      },
      "outputs": [],
      "source": [
        "train1 = train[['target']].copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2vwMRYgwCxs"
      },
      "outputs": [],
      "source": [
        "train1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fMYBWHB4N0g"
      },
      "outputs": [],
      "source": [
        "# Split data into parts\n",
        "x = train.drop(['target'], axis = 1)\n",
        "y = train['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWK1I8PBwCx2"
      },
      "outputs": [],
      "source": [
        "len(train)-len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeHRmt8n4N0h"
      },
      "outputs": [],
      "source": [
        "# Split data into Train Test\n",
        "X_train, X_test, y_train, y_test = x[len(train)-len(test):], x[:len(train)-len(test)], y[len(train)-len(test):], y[:len(train)-len(test)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpll6xDz4N0h"
      },
      "source": [
        "# Machine Learning Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--b5dI1L4N0h"
      },
      "source": [
        "# Non-Traditional Time Series Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-MG5g2V4N0h"
      },
      "source": [
        "### DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jV3ZqL64N0i"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "tree = DecisionTreeRegressor()\n",
        "model_tree = tree.fit(X_train, y_train)\n",
        "\n",
        "# Make prediction on X_test\n",
        "tree_pred = model_tree.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hk1yTop04N0i"
      },
      "outputs": [],
      "source": [
        "# feature importance for decision tree\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.barh(X_train.columns, model_tree.feature_importances_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGVD4sAy4N0i"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(y_test, label ='Actual Sales')\n",
        "plt.plot(tree_pred, label='DecisionTreeRegressor')\n",
        "plt.legend(loc='best')\n",
        "plt.title('DecisionTreeRegressor Prediction')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRIq48Od4N0j"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test, tree_pred )\n",
        "rmse = np.sqrt(mean_squared_error(y_test, tree_pred )).round(2)\n",
        "rmsle = np.sqrt(mean_squared_log_error(y_test, tree_pred)).round(2)\n",
        "msle = mean_squared_log_error(y_test, tree_pred).round(2)\n",
        "\n",
        "\n",
        "results = pd.DataFrame([['DecisionTree', mse, msle, rmse, rmsle]], columns = ['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_f0-Hnr4N0j"
      },
      "source": [
        "### KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03R8eIEg4N0k"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "neigh = KNeighborsRegressor(n_neighbors=1)\n",
        "# fit model no training data\n",
        "neigh.fit(X_train, y_train)\n",
        "\n",
        "# make predictions for test data\n",
        "neigh_pred = neigh.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# feature importance for decision tree\n",
        "plt.figure(figsize=(12,7))\n",
        "plt.barh(X_train.columns, neigh.feature_importances_)"
      ],
      "metadata": {
        "id": "x4Fhq5NAsYcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(y_test, label ='Actual Sales')\n",
        "plt.plot(neigh_pred, label='KNeighborsRegressor')\n",
        "plt.legend(loc='best')\n",
        "plt.title('KNeighborsRegressor Prediction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b5ga2_QilXjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORSI8VNk4N0k"
      },
      "outputs": [],
      "source": [
        "mse = mean_squared_error(y_test, neigh_pred )\n",
        "msle = mean_squared_log_error(y_test, neigh_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test, neigh_pred )).round(2)\n",
        "rmsle = np.sqrt(mean_squared_log_error(y_test, neigh_pred)).round(5)\n",
        "\n",
        "# model_results = pd.DataFrame([['lightGBM', mse, rmse]], columns = ['Model', 'MSE', 'RMSE'])\n",
        "model_results = pd.DataFrame([['KNN', mse, msle, rmse, rmsle]], columns = ['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results = results.append(model_results, ignore_index = True)\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RandomForestRegressor"
      ],
      "metadata": {
        "id": "kkdiDsNelg7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "# Initialize and fit the Random Forest Regressor\n",
        "forest = RandomForestRegressor()\n",
        "model_forest = forest.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "forest_pred = model_forest.predict(X_test)"
      ],
      "metadata": {
        "id": "r2hKk0NOll9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(y_test, label='Actual Sales')\n",
        "plt.plot(forest_pred, label='RandomForestRegressor')\n",
        "plt.legend(loc='best')\n",
        "plt.title('RandomForestRegressor Prediction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xfU3HZ_flwRx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mse = mean_squared_error(y_test, forest_pred)\n",
        "msle = mean_squared_log_error(y_test, forest_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "# Append the results to the DataFrame\n",
        "model_results = pd.DataFrame([['Random Forest', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results = results.append(model_results, ignore_index=True)\n",
        "results"
      ],
      "metadata": {
        "id": "8ZVEv2xYlxrl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Support Vector Regression (SVR)"
      ],
      "metadata": {
        "id": "f23SVbfcmF0V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "# Initialize and fit the SVR model\n",
        "svr = SVR()\n",
        "model_svr = svr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "svr_pred = model_svr.predict(X_test)"
      ],
      "metadata": {
        "id": "aJiDg2JtmMVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(y_test, label='Actual Sales')\n",
        "plt.plot(svr_pred, label='Support Vector Regression')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Support Vector Regression Prediction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hE8Jp3SdmNou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append the results to the DataFrame\n",
        "mse = mean_squared_error(y_test, svr_pred)\n",
        "msle = mean_squared_log_error(y_test, svr_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "model_results = pd.DataFrame([['SVR', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results = results.append(model_results, ignore_index=True)"
      ],
      "metadata": {
        "id": "9OFAEVXHmNlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosting"
      ],
      "metadata": {
        "id": "4ejfTTMmmtat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Initialize and fit the Gradient Boosting model\n",
        "gbr = GradientBoostingRegressor()\n",
        "model_gbr = gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "gbr_pred = model_gbr.predict(X_test)"
      ],
      "metadata": {
        "id": "_1gA-FJem2vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(y_test, label='Actual Sales')\n",
        "plt.plot(gbr_pred, label='Gradient Boosting')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Gradient Boosting Prediction')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ke9igzf3m4CD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Append the results to the DataFrame\n",
        "mse = mean_squared_error(y_test, gbr_pred)\n",
        "msle = mean_squared_log_error(y_test, gbr_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "model_results = pd.DataFrame([['Gradient Boosting', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results = results.append(model_results, ignore_index=True)"
      ],
      "metadata": {
        "id": "jSqhcFQOmxRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### XGBoost"
      ],
      "metadata": {
        "id": "6c8xEI0znFsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Initialize and fit the XGBoost model\n",
        "xgboost = xgb.XGBRegressor()\n",
        "model_xgboost = xgboost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "xgboost_pred = model_xgboost.predict(X_test)\n",
        "\n",
        "# Append the results to the DataFrame\n",
        "mse = mean_squared_error(y_test, xgboost_pred)\n",
        "msle = mean_squared_log_error(y_test, xgboost_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "model_results = pd.DataFrame([['XGBoost', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results = results.append(model_results, ignore_index=True)"
      ],
      "metadata": {
        "id": "6o8uz3jonHA9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression"
      ],
      "metadata": {
        "id": "P6rBf-mqnLnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# Initialize and fit the Linear Regression model\n",
        "linear_reg = LinearRegression()\n",
        "model_linear_reg = linear_reg.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on X_test\n",
        "linear_reg_pred = model_linear_reg.predict(X_test)\n",
        "\n",
        "# Append the results to the DataFrame\n",
        "mse = mean_squared_error(y_test, linear_reg_pred)\n",
        "msle = mean_squared_log_error(y_test, linear_reg_pred)\n",
        "rmse = np.sqrt(mse).round(2)\n",
        "rmsle = np.sqrt(msle).round(5)\n",
        "\n",
        "model_results = pd.DataFrame([['Linear Regression', mse, msle, rmse, rmsle]],\n",
        "                             columns=['Model', 'MSE', 'MSLE', 'RMSE', 'RMSLE'])\n",
        "results = results.append(model_results, ignore_index=True)"
      ],
      "metadata": {
        "id": "XCi5dxXenPNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px5fQiLs4N0k"
      },
      "source": [
        "## Models Comparison"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "D4I9RcCPnThC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqIJI8kx4N0l"
      },
      "source": [
        "## Model Evaluation (Backtests)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJzyU-dP4N0l"
      },
      "outputs": [],
      "source": [
        "# Backtests with KNN\n",
        "scores = {}\n",
        "\n",
        "for idx, period in enumerate(backtests):\n",
        "\n",
        "    _train = train.reset_index()[train.reset_index()['date_extracted'] < backtests[period][0]]\n",
        "    _test = train.reset_index()[(train.reset_index()['date_extracted'] >= backtests[period][0]) & (train.reset_index()['date_extracted'] <= backtests[period][1])]\n",
        "\n",
        "    Xtrain, ytrain = _train.set_index(['date_extracted']).drop(columns=['target']), _train.target\n",
        "    Xtest, ytest = _test.set_index(['date_extracted']).drop(columns=['target']), _test.target\n",
        "\n",
        "    knn_model = KNeighborsRegressor(n_neighbors=1).fit(Xtrain, ytrain)\n",
        "\n",
        "    ypred = knn_model.predict(Xtest)\n",
        "\n",
        "    scores[period] = np.sqrt(mean_squared_error(ytest, ypred))\n",
        "\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "CL5LyUScBhVT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfuyZwMQ4N0p"
      },
      "source": [
        "### predicting sales in our test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdiY19CV4N0q"
      },
      "outputs": [],
      "source": [
        "test_pred = neigh.predict(test)\n",
        "test_pred"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weekly_sum1['target'] = test_pred\n",
        "sub = weekly_sum1[['ID', 'target']]\n",
        "sub"
      ],
      "metadata": {
        "id": "LwULY8y7kX11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9M_q1aq4N0r"
      },
      "outputs": [],
      "source": [
        "# Save sample submission\n",
        "# test_sales[[ 'sales']].to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JtHj2gv4N0s"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "7679c2132d3f6ce38c9df14d554b39c06862b36a4e6689c81f9ae15bd0911d7d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}